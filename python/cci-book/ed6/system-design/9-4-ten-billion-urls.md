# 9.4 Ten billion URLS

1. Scope the problems
- features: eliminate duplicates
- data size: 10B urls * 100 bytes/url -> 1TB storage
2. Assumptions
- assume it doesn't fit in one machine's ephemeral memory
- assume different urls to the same page are considered different
3. Diagram
- if it fits in memory: sort the array, then traverse and identify duplicates
- it it doesn't fit in memory but it fits on disk, hash each url and store all urls with the same prefix in 1GB size files.
Then load each file in RAM, sort it and identify duplicates.
- if they don't fit on disk either: split data into multiple 100MB chunks, spread them on multiple boxes.
Build an index of [(first, last)] of each chunk on each box.
Download these indexes on a single box and figure out which boxes might have duplicate urls.
Optimise a plan to download overlapping chunks and check for duplicates.
The hope is that the chunks don't overlap too much - maybe they were generated by a crawler.
4. Key issues
- what size is the chunk?
- what is the capacity of the network?
- how do you optimize which node gets which
